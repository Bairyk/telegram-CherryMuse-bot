Optimized Prompt (Split into Three Parts)
Part 1: Telegram Bot Setup and Deployment
You are an expert software architect and developer specializing in building scalable AI-powered chat applications using Python, Telegram bots, and open-source AI models. Your goal is to provide an ultra-detailed, step-by-step guide for Part 1: Telegram Bot Setup and Deployment as part of a system mimicking a creative role-playing chat tool like a "virtual companion for unrestricted storytelling and idea generation" (e.g., similar to apps that allow free-form creative interactions without content restrictions, fostering imagination in stories, art, and animations). The system is designed for future scalability (user auth, price tiers, crypto payments, multi-user support, error handling, feedback, scalability).
System Overview (Context for All Parts):

Telegram Bot: Python using python-telegram-bot library, hosted on GitHub with CI/CD to Railway (free tier). Handles user chats, processes text/image/video prompts, stores key elements temporarily, forwards requests to Hugging Face Spaces for generation, and logs interactions.
External AI Server: Hugging Face Spaces free tier (ephemeral, ZeroGPU for GPU tasks). Runs Llama-based model (text), Stable Diffusion (images), AnimateDiff (30s video clips)—all minimal tuning, from Hugging Face.
Storage: SQLite for chat metadata, free/low-cost cloud for media (delete non-key items after 24 hours or immediately).
Logging/Analytics: Track prompt/image/video popularity (e.g., themes, frequency), errors, and suggest improvements (e.g., new features). Logs in SQLite/JSON, anonymized for privacy.
Budget: Hugging Face Spaces and Railway free tiers (under $5/mo, flexible for cheap add-ons).

Part 1 Scope: Telegram Bot Setup and Deployment
Focus on:

Setting up a GitHub repo with CI/CD for Railway deployment.
Writing a Python script using python-telegram-bot to:

Handle user commands (/start, /help, /feedback).
Process text/image/video inputs for role-playing chats.
Forward requests to a Hugging Face Spaces API (placeholder for Part 2).
Log interactions (e.g., prompt themes, errors) to SQLite for analytics.


Deploying to Railway free tier with CI/CD.
Basic error handling (e.g., retries for API failures).

Instructions:

Planning and Comparisons: Briefly justify choices:

Why python-telegram-bot (vs. aiogram)?
Why Railway free tier for bot hosting (vs. Render, Heroku)?
SQLite for initial logging (vs. file-based).


Implementation:

Provide concise code snippets (e.g., bot setup, command handlers, SQLite logging).
Detail steps: Create GitHub repo, write bot script, set up Railway CI/CD, test locally.
Include configuration (e.g., requirements.txt, .env for bot token).
Add troubleshooting (e.g., Railway deployment errors, bot permissions).


Testing: Steps to verify bot responds to commands, logs data, and preps for API calls.
Use markdown, assume intermediate Python knowledge, and keep code concise to avoid chat limits. Include few-shot examples (e.g., handling a text prompt). Emphasize safety, efficiency, and creative freedom.

Part 2: AI Server Setup on Hugging Face Spaces
You are an expert software architect and developer specializing in building scalable AI-powered chat applications. This is Part 2: AI Server Setup on Hugging Face Spaces for the same system (see System Overview above).
Part 2 Scope: AI Server Setup on Hugging Face Spaces
Focus on:

Setting up Hugging Face Spaces free tier (ephemeral, ZeroGPU) to host:

Text-to-text: Llama-based model (e.g., meta-llama/Llama-3-8B).
Image generation: Stable Diffusion (e.g., runwayml/stable-diffusion-v1-5).
Video generation: AnimateDiff (e.g., guoyww/animatediff, 30s clips).


Using Gradio/Streamlit to create an API endpoint for bot requests.
Handling free tier limits (e.g., queues, ephemeral storage) with retries/fallbacks.
Minimal tuning: Download models from Hugging Face, run via Transformers/Diffusers.

Instructions:

Planning and Comparisons: Briefly justify:

Model choices (e.g., Llama 3 vs. Mistral, SD 1.5 vs. XL, AnimateDiff vs. others) for ease and free tier fit.
Gradio vs. Streamlit for API.
Handling ZeroGPU/ephemeral limits (vs. PRO tier).


Implementation:

Steps: Create HF Space, clone model repos, write app.py (Gradio-based API), deploy.
Concise code snippets: Load models, expose endpoints (e.g., /text, /image, /video).
Configuration: requirements.txt, README for Space.
Troubleshooting: Queue delays, model load errors, GPU allocation.


Testing: Verify API endpoints work, handle bot requests, and manage quotas.
Use markdown, keep code concise, and include few-shot examples (e.g., image generation call). Ensure compatibility with bot from Part 1.

Part 3: Storage, Logging, and Future Hooks
You are an expert software architect and developer specializing in building scalable AI-powered chat applications. This is Part 3: Storage, Logging, and Future Hooks for the same system (see System Overview above).
Part 3 Scope: Storage, Logging, and Future Hooks
Focus on:

Storage: SQLite for chat metadata (prompts, user IDs), free cloud (e.g., GitHub for temp media sync). Delete non-key items after 24 hours/immediately.
Logging/Analytics: Log prompt/image/video interactions (e.g., themes like "fantasy," frequency, success/failure, latency). Aggregate for insights (e.g., popular features, errors, suggested additions like new styles). Store in SQLite/JSON, exportable for review.
Future Hooks: Placeholders for user auth (Telegram IDs, expandable to JWT), price tiers (rate limits), crypto payments, feedback (/feedback command), scalability (e.g., cloud migration).
Integration with bot (Part 1) and AI server (Part 2).

Instructions:

Planning and Comparisons: Briefly justify:

SQLite vs. file-based for logs/storage.
GitHub vs. other free cloud for media (e.g., AWS S3 free tier).
Analytics approach (e.g., simple counts vs. ML-based trends).


Implementation:

Steps: Set up SQLite schema, write logging code (bot + server), implement deletion cron, add hooks.
Concise code snippets: Log prompt (e.g., theme, timestamp), aggregate stats, auth placeholders.
Configuration: SQLite setup, cron jobs for deletion.
Troubleshooting: Log corruption, storage limits, cron failures.


Testing and Optimization: Verify storage/logs, check analytics (e.g., query top prompts), optimize for free tier.
Use markdown, keep code concise, and include few-shot examples (e.g., logging a video gen). Ensure privacy (anonymized logs) and compatibility with Parts 1-2.


Key Improvements:

Split for Claude Limits: Divided Implementation Phase into three parts (Bot, AI Server, Storage/Logging) to fit token constraints, each self-contained but linked to the system overview.
Logging/Analytics Enhanced: Explicitly included analytics requirements (popularity, errors, suggestions) across all parts, with Part 3 focusing on aggregation and insights.
Free Tier Focus: Emphasized Hugging Face Spaces free tier and Railway, with strategies for handling limits (e.g., retries, quantization).
Conciseness: Streamlined code instructions to minimize token usage while retaining detail, using few-shot examples for clarity.

Techniques Applied: Task decomposition (three parts); chain-of-thought for comparisons; role assignment; few-shot for code; constraint optimization (free tier, brevity).
Pro Tip: Run each part separately in Claude to avoid limits. Save outputs as markdown files, then combine for a full guide. Test Part 1 first (bot) locally, then integrate with Part 2 (API) and Part 3 (logs). Use Claude’s output to debug (e.g., check logs for API errors). If you hit quotas on Spaces, Part 2 includes fallbacks.
